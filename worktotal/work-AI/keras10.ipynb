{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"keras10.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOYMNQkzA0BS8CMp6g2KDBF"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"ZlEaUaMqNxOA","colab_type":"code","colab":{}},"source":["# 단어 자동 완성 만들기\n","# RNN 모델을 이용하여 단어를 자동 완성하는 프로그램을 만들어 보자.\n","# 영문자 4개로 구성된 단어를 학습시켜, 3글자만 주어지면 나머지 한 글자를 추천하여 단어를\n","# 완성하는 프로그램이다.\n","# 학습시킬 데이터는 영문자로 구성된 임의의 단어를 사용한 것이고, 한 글자 한 글자를 하나의\n","# 단계로 볼 것이다. 그러면 한 글자가 한 단계의 입력값이 되고, 총 글자 수가 전체 단계가\n","# 된다.\n","\n","import tensorflow as tf\n","import numpy as np\n","\n","char_arr = ['a', 'b', 'c', 'd', 'e', 'f', 'g',\n","            'h', 'i', 'j', 'k', 'l', 'm', 'n',\n","            'o', 'p', 'q', 'r', 's', 't', 'u',\n","            'v', 'w', 'x', 'y', 'z']\n","\n","# 알파벳 글자 순서에 해당하는 인덱스 번호와 글자를 딕셔너리에 저장\n","# {'a': 0, 'b': 1, 'c': 2, ..., 'j': 9, 'k', 10, ...}\n","num_dic = {n:i for i, n in enumerate(char_arr)}\n","print(num_dic)\n","dic_len = len(num_dic)    \n","print(dic_len)            # 26\n","\n","# 학습에 사용할 단어를 배열로 저장\n","# 다음 배열은 입력갑과 출력값으로 다음처럼 사용할 것이다.\n","# wor -> X, d -> Y\n","# woo -> X, d -> Y\n","seq_data = ['word','wodd','deep','dive','cold','load','love','kiss','kind']\n","\n","# 학습에 사용할 단어들을 one-hot-encoding으로 변환하는 함수\n","def make_batch(seq_date):\n","  input_batch = []  # 학습에 사용할 단어의 처음 세글자의 알파벳 인덱스 번호를 저장할 배열\n","  target_batch = [] # 학습에 사용할 단어의 마지막 글자의 알파벳 인덱스 번호를 저장할 배열\n","\n","  for seq in seq_data:\n","    # 1. 입력값용으로, 단어의 처름 세 글자의 알파벳 인덱스를 구한 배열을 만든다.\n","    # [22,14,17][22,14,14][3,4,4][3,8,21]...\n","    input = [num_dic[n] for n in seq[:-1]]\n","\n","    # 2. 출력값용으로, 마지막 글자의 알파벳 인덱스를 구한다.\n","    # 3,3,15,4,3,...\n","    target = num_dic[seq[-1]]\n","\n","    # 3. 입력된값을 원-핫 인코딩으로 변환한다.\n","    # [[ 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","    # [ 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n","    # [ 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n","    input_batch.append(np.eye(dic_len)[input])\n","\n","    target_batch.append(target)\n","\n","  return input_batch, target_batch\n","\n","#############\n","# 옵션 설정\n","#############\n","learning_rate = 0.01      # 학습률\n","n_hidden =128             # 은닉층의 노드 갯수\n","total_epoch = 30          # 학습 횟수\n","\n","# 단어의 전체 중 처음 3글자를 단계적으로 학습할 것이므로, n_step은 3이 된다.\n","# 타입 스텝: [1,2,3] => 3\n","# RNN을 구성하는 시퀀스의 갯수\n","n_step = 3\n","\n","# 입력값 크기. 알파벳에 대한 one-hot 인코딩이므로 26개 가 된다.\n","# c => [0 0 1 0 0 0 0 0 ...0]\n","# 출력값도 입력값과 마차나지로 26개의 알파벳으로 분류한다.\n","n_input = n_class = dic_len\n","\n","###################\n","# 신경망 모델 구성\n","###################\n","X = tf.placeholder(tf.float32, [None, n_step, n_input])\n","\n","# 비용함수에 sparse_softmax_cross_entropy_with_logits 을 사용하므로\n","# 출력값과의 계산을 위한 원본값의 형태는 one-hot vector가 아니라 인덱스 숫자를 그대로 사용하기 때문에\n","# 다음처럼 하나의 값만 있는 1차원 배열을 입력값으로 받는다.\n","# [3] [3] [15] [4] ...\n","# 기존처럼 one-hot 인코딩을 사용한다면 입력값의 형태는 [None, n_class] 여야한다.\n","Y = tf.placeholder(tf.int32, [None])\n","\n","W = tf.Variable(tf.random_normal([n_hidden, n_class]))\n","b = tf.Variable(tf.random_normal([n_class]))\n","\n","\n","# RNN셀을 생성\n","# 여러 셀을 조합해 심층 신경망을 만들기 위해서 2개의 RNN 셀을 생성\n","cell1 = tf.nn.rnn_cell.BasicLSTMCell(n_hidden)\n","\n","# 과적합 방지를 위한 Dropout 기법을 사용\n","cell1 = tf.nn.rnn_cell.DropoutWrapper(cell1, output_keep_prob=0.5)\n","\n","# 여러개의 셀을 조합해서 사용하기 위해 셀을 추가로 생성한다.\n","cell2 = tf.nn.rnn_cell.BasicLSTMCell(n_hidden)\n","\n","# 여러개의 셀을 조합한 RNN셀을 생성\n","# 위에서 만든 2개의 RNN셀을 MultiRNNCell() 함수를 사용하여 조합한다.\n","multi_cell = tf.nn.rnn_cell.MultiRNNCell([cell1, cell1])\n","\n","# tf.nn.dynamic_rnn 함수를 이용해 심층 순환 신경망, 즉 Deep RNN을 만든다.\n","# time_major = True\n","outputs, states = tf.nn.dynamic_rnn(multi_cell, X, dtype=tf.float32)\n","\n","# 최종 출력층\n","# 최종 결과는 one-hot 인코딩 형식으로 만듭니다.\n","outputs = tf.transpose(outputs, [1, 0, 2])\n","outputs = outputs[-1]\n","model = tf.matmul(outputs, W) + b\n","\n","# 손실 함수로는 sparse_sofrmax_cross_entropy_with_logits 를,\n","# 최적화 함수로는 AdamOptimizer를 설멍하여 신경망 모델 구성을 마무리 한다.\n","cost = tf.reduce_mean(\n","          tf.nn.sparse_sofrmax_cross_entropy_with_logits(\n","              logits=model, labels=Y))\n","optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n","\n","####################\n","# 신경망 모델 학습\n","####################\n","sess = tf.Session()\n","sess.run(tf.global_variables_initializer())\n","\n","# make_batch 함수를 이용하여 seq_data에 저장한 단어들을 입력값(처음 세 글자)과 실측값(마지막 한 글자)으로\n","# 분리하고, 이 값들을 최적화 함수를 실행하는 코드에 넣어 신경망를 학습시킨다. \n","input_batch, target_batch = make_batch(seq_data)\n","\n","# 30회 학습\n","for epoch in range(total_epoch):\n","  _, loss = sess.run([optimizer, cost],\n","                  feed_dict = {X: input_batch, Y:target_batch})\n","  print('Epoch:', '%04d' %(epoch + 1), 'cost= ', '{:.6f}'.format(loss))\n","\n","print('최적화 완료')\n","\n","##############\n","# 결과 확인\n","##############\n","# 레이블 값이 정수이므로 예측값도 정수로 변경해준다.\n","prediction = tf.cast(tf.argmax(model, 1), tf.int32)\n","\n","# one-hot 인코딩이 아니므로 입력값을 그대로 비교한다.\n","prediction_check = tf.equal(prediction, Y)\n","accuracy = tf.reduce_mean(tf.cast(prediction_check, tf.float32))\n","\n","# 학습에 사용한 단어들을 넣고 예측 모델을 돌린다.\n","input_batch, target_batch = make_batch(seq_data)\n","\n","predict, accuracy_val = sess.run([prediction, accuracy],\n","                                 feed_dick={X: input_batch, Y: target_batch})\n","\n","# 모델이 예측한 값들을 가지고, 각각의 값에 해당하는 인덱스의 알파벳을 가져와서 예측한 단어를 출력\n","predict_words = []\n","for idx, val in enumerate(seq_data):\n","  last_char = char_arr[predict[idx]]\n","  predict_words.append(val[:3] + last_char)\n","\n","print('\\n=== 예측 결과 ===')\n","print('입력값:', [w[:3] + ' ' for w in seq_data])\n","print('예측값:', predict_words)\n","print('정확도:', accuracy_val)\n","\n","# replaceholder 부분 수정 필요 (74, 81)"],"execution_count":null,"outputs":[]}]}