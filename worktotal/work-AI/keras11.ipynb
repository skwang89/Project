{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"keras11.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPyOOmKdTKf7FQ/TPnzPDv7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"OsBvB72zTIz3","colab_type":"code","colab":{}},"source":["# 영어 단어를 핚국어 단어로 번역하는 프로그램\n","\n","import tensorflow as tf\n","import numpy as np\n","\n","# 영어 알파벳과 한글들을 나열한 뒤 한 글자씩 배열에 집어 넣는다.\n","# 그런 다음 배열에 넣은 글자들을 딕셔너리 형태로 변경 합니다.\n","# Sequence to Sequence 모델에는 특수한 심볼이 몇개 필요하다\n","# S: 디코딩 입력의 시작을 나타내는 심볼\n","# E: 디코딩 출력을 끝을 나타내는 심볼\n","# P: 현재 배치 데이터의 time step 크기보다 작은 경우 빈 시퀀스를 채우는 심볼\n","# 예) 현재 배치 데이터의 최대 크기가 4 인 경우\n","# word -> ['w', 'o', 'r', 'd']\n","# to -> ['t', 'o', 'P', 'P']\n","char_arr = [c for c in 'SEPabcdefghijklmnopqrstuvwxyz단어나무놀이소녀키스사랑']\n","num_dic = {n: i for i, n in enumerate(char_arr)}\n","dic_len = len(num_dic)\n","print(num_dic)\n","\n","# 영어를 한글로 번역하기 위한 학습 데이터\n","seq_data = [['word', '단어'], ['wood', '나무'],\n","           ['game', '놀이'], ['girl', '소녀'],\n","           ['kiss', '키스'], ['love', '사랑']]\n","\n","# 입력 단어와 출력 단어를 한 글자씩 떼어낸 뒤 배열로 만든후에\n","# one-hot 인코딩 형식으로 만들어 주는 함수\n","# 데이터는 총 3개로 구성된다(인코더의 입력값, 디코더의 입력값과 출력값)\n","def make_batch(seq_data):\n","  input_batch = [] # 인코더 셀의 입력값\n","  output_batch = [] # 디코더 셀의 입력값\n","  target_batch = [] # 디코더 셀의 출력값\n","  \n","  for seq in seq_data:\n","\n","    #1.인코더 셀의 입력값. 입력단어의 글자들을 한글자씩 떼어 배열로 만든다.\n","    input = [num_dic[n] for n in seq[0]]\n","\n","    #2.디코더 셀의 입력값. 시작을 나타내는 S 심볼을 맨 앞에 붙여준다.\n","    output = [num_dic[n] for n in ('S' + seq[1])]\n","\n","    #3.학습을 위해 비교할 디코더 셀의 출력값. 끝나는 것을 알려주기 위해 마지막에 E 를 붙인다.\n","    target = [num_dic[n] for n in (seq[1] + 'E')]\n","\n","    #위에서 맊들어진 데이터를 one-hot 인코딩합니다.\n","    input_batch.append(np.eye(dic_len)[input])\n","    output_batch.append(np.eye(dic_len)[output])\n","\n","    # 출력값맊 one-hot 인코딩이 아닌 인덱스 번호를 리턴 (sparse_softmax_cross_entropy_with_logits 사용)\n","    target_batch.append(target)\n","\n","  return input_batch, output_batch, target_batch\n","\n","#############\n","# 옵션 설정\n","#############\n","learning_rate = 0.01 # 학습률\n","n_hidden = 128 # 은닉층의 노드 갯수\n","total_epoch = 100 # 학습 횟수\n","\n","# 입력과 출력의 형태가 one-hot 인코딩으로 같으므로 크기도 같다.\n","n_class = n_input = dic_len\n","\n","####################\n","# 신경망 모델 구성\n","####################\n","# Seq2Seq 모델은 인코더의 입력과 디코더의 입력의 형식이 같다.\n","# 입력값들은 one-hot 인코딩을 사용하고, 디코더의 출력값은 인덱스 숫자를\n","# 그대로 사용하기 때문에 입력값의 랭크(차원)가 하나 더 높다.\n","# [batch size, time steps, input size] : 인코더와 디코더의 입력값 형식\n","enc_input = tf.placeholder(tf.float32, [None, None, n_input])\n","dec_input = tf.placeholder(tf.float32, [None, None, n_input])\n","\n","# [batch size, time steps] : 디코더의 출력값 형식\n","targets = tf.placeholder(tf.int64, [None, None])\n","\n","# RNN 모델을 위한 셀을 구성\n","# : 인코더 셀과 디코더 셀로 구성\n","# 1.인코더 셀을 구성한다.\n","with tf.variable_scope('encode'):\n","  # 셀은 기본 셀을 사용 하였고, 각 셀에 드롭아웃을 적용\n","  enc_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)\n","  enc_cell = tf.nn.rnn_cell.DropoutWrapper(enc_cell, output_keep_prob=0.5)\n","  outputs, enc_states = tf.nn.dynamic_rnn(enc_cell, enc_input,\n","                                          dtype=tf.float32)\n","# 2.디코더 셀을 구성한다.\n","with tf.variable_scope('decode'):\n","\n","  # 셀은 기본 셀을 사용 하였고, 각 셀에 드롭아웃을 적용\n","  dec_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)\n","  dec_cell = tf.nn.rnn_cell.DropoutWrapper(dec_cell, output_keep_prob=0.5)\n","  # Seq2Seq 모델은 인코더 셀의 최종 상태값을\n","  # 디코더 셀의 초기 상태값으로 넣어주는 것이 핵심 아이디어\n","  # (텐서플로의 dynamic_rnn에 initial_state=enc_states 옵션 사용)\n","  outputs, dec_states = tf.nn.dynamic_rnn(dec_cell, dec_input,\n","                                          initial_state=enc_states,\n","                                          dtype=tf.float32)\n","\n","# 출력층 생성\n","# 출력층을 위해 layers 모듈의 dense함수를 사용\n","model = tf.layers.dense(outputs, n_class, activation=None)\n","\n","# 비용함수\n","cost = tf.reduce_mean(\n","        tf.nn.sparse_softmax_cross_entropy_with_logits(\n","          logits=model, labels=targets))\n","\n","# 최적화 함수 : AdamOptimizer 사용\n","optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n","\n","####################\n","# 신경망 모델 학습\n","####################\n","# feed_dict 로 전달하는 학습 데이터에\n","# 인코더의 입력값, 디코더의 입력값과 출력값 이렇게 3개를 넣어서 학습 시킨다.\n","sess = tf.Session()\n","sess.run(tf.global_variables_initializer())\n","\n","input_batch, output_batch, target_batch = make_batch(seq_data)\n","\n","# 100회 학습\n","for epoch in range(total_epoch):\n","  _, loss = sess.run([optimizer, cost],\n","                  feed_dict={enc_input: input_batch, # 인코더의 입력값\n","                              dec_input: output_batch, # 인코더의 출력값\n","                              targets: target_batch}) # 디코더의 입력값\n","  print('Epoch:', '%04d' %(epoch + 1),\n","      'cost =', '{:.6f}'.format(loss))\n","print('최적화 완료!')\n","\n","###############\n","# 번역 테스트\n","###############\n","# 단어를 입력받아 번역 단어를 예측하고 디코딩하는 함수\n","def translate(word):\n","  # 이 모델은 입력값과 출력값 데이터로 [영어단어, 한글단어] 사용하지만,\n","  # 예측시에는 한글단어를 알지 못하므로, 디코더의 입출력값을 의미 없는 값인 P 값으로 채운다.\n","  # 입력으로 'word'를 받았다면, seq_data=['word', 'PPPP']로 구성될 것이다.\n","  seq_data = [word, 'P' * len(word)]\n","\n","  # make_batch([seq_data]) 함수 호출후 다음의 결과를 리턴 받는다.\n","  # input_batch는 ['w','o','r','d'], output_batch는 ['P','P','P','P'] 글자들의 인덱스를 one-hot 인코딩핚 값을 리턴 받는다\n","  # target_batch는 ['P','P','P','P']의 각 글자의 인덱스인 [2,2,2,2]를 리턴 받는다.\n","  input_batch, output_batch, target_batch = make_batch([seq_data])\n","\n","  # 예측 모델을 실행\n","  # 결과가 [batch size, time step, input] 으로 나오기 때문에,\n","  # 2번째 차원인 input 차원을 argmax 로 취해 가장 확률이 높은 글자를 예측 값으로 만든다.\n","  prediction = tf.argmax(model, 2)\n","\n","  result = sess.run(prediction,\n","                feed_dict={enc_input: input_batch, # 인코더의 입력값\n","                        dec_input: output_batch, # 인코더의 출력값\n","                        targets: target_batch}) # 디코더의 입력값\n","  # 예측 결과는 글자의 인덱스를 뜻하는 숫자이므로 각 숫자에 해당하는 글자를 가져와 배열을 맊든다.\n","  decoded = [char_arr[i] for i in result[0]]\n","\n","  # 출력의 끝을 의미하는 'E' 이후의 글자들을 제거하고 문자열로 만든다.\n","  end = decoded.index('E')\n","  translated = ''.join(decoded[:end])\n","\n","  return translated\n","\n","  print('\\n=== 번역 테스트 ===')\n","\n","# 번역함수(translate())를 사용하여 몇 단어들의 번역 결과를 리턴 받는다.\n","print('word ->', translate('word'))\n","print('wodr ->', translate('wodr'))\n","print('love ->', translate('love'))\n","print('loev ->', translate('loev'))\n","print('abcd ->', translate('abcd'))\n","\n","# repaceholder 수정 필요 (70, 71)"],"execution_count":null,"outputs":[]}]}